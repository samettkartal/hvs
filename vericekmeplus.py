import os
import csv
import time
import random # Rastgele bekleme sÃ¼releri iÃ§in bu kÃ¼tÃ¼phaneyi ekliyoruz
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# ==============================================================================
# 1. AYARLAR
# ==============================================================================
sirketler = [
    # E-Ticaret
    "trendyol", "hepsiburada", "getir", "n11", "ciceksepeti",
    # Kargo
    "yurtici-kargo", "aras-kargo", "mng-kargo", "ptt-kargo",
    # Havayolu
    "thy", "pegasus", "anadolujet",
    # Perakende & Market
    "migros", "a101", "bim-market",
    # BankacÄ±lÄ±k
    "ziraat-bankasi", "garanti-bbva", "is-bankasi",
    # DiÄŸer PopÃ¼ler Åirketler
    "sahibinden-com", "arcelik", "vestel", "spotify"
]

csv_dosya = "toplu_sikayetler_playwright.csv"

# ==============================================================================
# 2. DOSYA YÃ–NETÄ°MÄ°
# ==============================================================================
if not os.path.exists(csv_dosya):
    print(f"'{csv_dosya}' dosyasÄ± bulunamadÄ±, baÅŸlÄ±klar eklenerek oluÅŸturuluyor...")
    with open(csv_dosya, mode="w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["BaÅŸlÄ±k", "Åikayet"])

# ==============================================================================
# 3. YARDIMCI FONKSÄ°YONLAR
# ==============================================================================
def get_complaint_links(page, sirket_slug):
    try:
        page.wait_for_selector("a.complaint-layer", timeout=7000)
        links = page.locator("a.complaint-layer").all()
        hrefs = [link.get_attribute("href") for link in links]
        full_links = [
            f"https://www.sikayetvar.com{href}"
            for href in hrefs
            if href and href.startswith(f"/{sirket_slug}/")
        ]
        return full_links
    except PlaywrightTimeoutError:
        print("  - Bu sayfada ÅŸikayet linki bulunamadÄ± veya sayfa yÃ¼klenemedi.")
        return []

def get_title_and_first_paragraph(page):
    try:
        page.wait_for_selector("h1", timeout=10000)
        title = page.locator("h1").inner_text().strip()
    except PlaywrightTimeoutError:
        title = "âŒ BaÅŸlÄ±k bulunamadÄ±"

    try:
        paragraph = page.locator("xpath=//h1/following::p[1]").inner_text().strip()
    except Exception:
        paragraph = "âŒ Paragraf alÄ±namadÄ±"

    return title, paragraph

# ==============================================================================
# 4. ANA Ä°ÅLEM BLOÄU
# ==============================================================================
with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    
    # âœ¨âœ¨âœ¨ YENÄ° YAKLAÅIM BURADA BAÅLIYOR âœ¨âœ¨âœ¨
    # 1. GerÃ§ek bir tarayÄ±cÄ±nÄ±n kimliÄŸini taklit ediyoruz (User-Agent).
    # Bu, Cloudflare'a "Ben normal bir Chrome kullanÄ±cÄ±sÄ±yÄ±m" demenin en basit yoludur.
    context = browser.new_context(
        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
    )
    
    page = context.new_page()

    # Her bir ÅŸirket iÃ§in ana dÃ¶ngÃ¼yÃ¼ baÅŸlat
    for sirket_slug in sirketler:
        print("\n" + "="*60)
        print(f"ğŸ¢ Ä°ÅLEM BAÅLADI: '{sirket_slug.upper()}' ÅÄ°RKETÄ° Ä°Ã‡Ä°N VERÄ° Ã‡EKÄ°LÄ°YOR")
        print("="*60)
        
        for sayfa in range(1, 11):
            base_url = f"https://www.sikayetvar.com/{sirket_slug}?page={sayfa}"
            
            print(f"\nğŸ“„ Sayfa {sayfa} ({sirket_slug}) taranÄ±yor...")
            try:
                page.goto(base_url, timeout=30000)
                
                # 2. Daha insan benzeri, rastgele beklemeler ekliyoruz.
                # Botlar genellikle sabit sÃ¼relerde bekler, insanlar ise beklemez.
                time.sleep(random.uniform(2.5, 4.5)) # 2.5 ile 4.5 saniye arasÄ±nda rastgele bir sÃ¼re bekle

                links = get_complaint_links(page, sirket_slug)
                print(f"ğŸ”— {len(links)} geÃ§erli ÅŸikayet linki bulundu.")

                for link in links:
                    try:
                        page.goto(link, timeout=30000)
                        time.sleep(random.uniform(1.5, 3.0)) # Her ÅŸikayet sayfasÄ± arasÄ±nda da kÄ±sa bir sÃ¼re bekle
                        
                        title, para = get_title_and_first_paragraph(page)
                        
                        if "âŒ" not in para:
                            print(f"  âœ… BaÅŸarÄ±yla Ã§ekildi: {title[:70]}...")
                        else:
                            print(f"  âš ï¸ Paragraf Ã§ekilemedi: {title[:70]}...")

                        with open(csv_dosya, mode="a", encoding="utf-8", newline="") as f:
                            writer = csv.writer(f)
                            writer.writerow([title, para])
                    except Exception as e:
                        print(f"  âŒ HATA (Link iÅŸlenirken: {link}): {str(e)}")
            except PlaywrightTimeoutError:
                 print(f"  âŒ KRÄ°TÄ°K HATA: Sayfa yÃ¼klenirken zaman aÅŸÄ±mÄ±na uÄŸradÄ±. Muhtemelen Cloudflare engeline takÄ±ldÄ±.")
                 print("  â¡ï¸ Bir sonraki ÅŸirkete geÃ§iliyor...")
                 break
            except Exception as e:
                print(f"  âŒ KRÄ°TÄ°K HATA (Sayfa {sayfa} aÃ§Ä±lamadÄ±): {str(e)}")
                break

    print("\n\nğŸ‰ TÃ¼m iÅŸlemler bitti. TarayÄ±cÄ± kapatÄ±lÄ±yor.")
    browser.close()